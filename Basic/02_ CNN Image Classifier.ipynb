{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruit Detection (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all te req lib\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 28,28\n",
    "\n",
    "#defining the data directories\n",
    "train_data_dir= 'data/train'\n",
    "validation_data_dir= 'data/validation'\n",
    "n_training_sample= 400\n",
    "n_validation_sample= 100\n",
    "epochs=20\n",
    "batch_size=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(), \n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the optimizer and metrics\n",
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 984 images belonging to 2 classes.\n",
      "Found 328 images belonging to 2 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 40 steps, validate for 10 steps\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 0.4291 - accuracy: 0.7437 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.0516 - accuracy: 0.9848 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 7.8160e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 5.3317e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 3.6359e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.9109e-04 - accuracy: 1.0000 - val_loss: 7.6974e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 1.1421e-04 - accuracy: 1.0000 - val_loss: 1.4193e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 7.4885e-05 - accuracy: 1.0000 - val_loss: 1.8149e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 6.6071e-05 - accuracy: 1.0000 - val_loss: 4.1287e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 4.1957e-05 - accuracy: 1.0000 - val_loss: 1.5004e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 4.2614e-05 - accuracy: 1.0000 - val_loss: 1.3332e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 3.0077e-05 - accuracy: 1.0000 - val_loss: 7.3250e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 2.4364e-05 - accuracy: 1.0000 - val_loss: 1.2925e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 2.2199e-05 - accuracy: 1.0000 - val_loss: 7.1878e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 2.1601e-05 - accuracy: 1.0000 - val_loss: 1.0170e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 2s 39ms/step - loss: 1.3765e-05 - accuracy: 1.0000 - val_loss: 7.5143e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.2274e-05 - accuracy: 1.0000 - val_loss: 3.7382e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.2430e-05 - accuracy: 1.0000 - val_loss: 3.0464e-05 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 1.0997e-05 - accuracy: 1.0000 - val_loss: 3.9766e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1269d196780>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_training_sample // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_sample // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "pred= image.load_img('data/test/pm 1.jpg', target_size=(28,28))\n",
    "pred=image.img_to_array(pred)\n",
    "pred= np.expand_dims(pred, axis=0)\n",
    "result= model.predict(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fruit in the image is Pomegranate\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    answer='Apple'\n",
    "else:\n",
    "    answer='Pomegranate'\n",
    "print(\"The fruit in the image is\",answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
